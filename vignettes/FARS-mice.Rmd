---
title: "Using mice for multiply imputed FARS data"
author: "Stas Kolenikov"
date: "2025-07-28"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{FARS-2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include=FALSE}
library(mice)     # target package to deal with multiple imputation
library(haven)    # to read SAS files
library(dplyr)    # manipulations
library(janitor)  # occasional tabulations
library(stringr)  # string processing
library(tidyr)    # reshape; functionality is imported to dplyr
library(labelled) # to deal with as.factor() gracefully
library(furrr)    # extension to purrr with future parallel processing

# helper function
mipo_to_indicative <- function(mipo.object) {
  this_df <- mipo.object$pooled
  
  this_df %>%  
    dplyr::arrange(estimate) %>% 
    dplyr::slice(c(1,nrow(this_df))) -> estimate_min_max
  this_df %>%  
    dplyr::arrange(fmi) %>% 
    dplyr::slice(c(1,nrow(this_df))) -> fmi_min_max
  this_df %>%  
    dplyr::arrange(df) %>% 
    dplyr::slice(c(1,nrow(this_df))) -> df_min_max
  
  dplyr::distinct(
    dplyr::bind_rows(
      estimate_min_max, fmi_min_max, df_min_max
    )
  ) %>% dplyr::arrange(term) %>% 
    dplyr::mutate(term=as.character(term), 
      term = term %>% 
        stringr::str_replace_all(stringr::fixed("as.factor("), "") %>% 
        stringr::str_replace_all(stringr::fixed(")"), "==")) 
}
```

## FARS data

[Fatality Analysis Reporting System](https://www.nhtsa.gov/research-data/fatality-analysis-reporting-system-fars) 
is a census of all police-reported 
fatal crashes in the United States, assembled and maintained
by the [National Highway Traffic Safety Administration](https://www.nhtsa.gov/) (NHTSA), 
part of the [U.S. Department of Transportation](https://www.dot.gov/).
Each year, FARS public data use files are released
as a set of 20+ files, linkable by specific
stable IDs across the files (see "proper" p. 14 / overall p. 19
of the [Analytic Users Manual](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/813706)).

One of the FARS variables is multiply imputed:
blood alcohol content (BAC). The variable is missing in about 25%
of the raw data, but given its high policy relevance,
a custom imputation model for it was built at the time
that can be considered "multiple imputation adolescence"
([Rubin, Schaffer and Subramanian (1998)](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/808816)): 
by that time, the idea had been in the literature for two decades and quite well studied,
but the software to work with multiply imputed data
was not yet standardized and streamlined, leaving MI implentations to be
artisan data solutions. The FARS BAC imputation model involves:

1. Separating cases by vehicle class (passenger cars;
utility vehicles; light trucks and vans; minivans; 
medium and heavy trucks; motorcycles; non-motorists; 
other/miscellaneous vehicles).
2. Loglinear modeling of the dichotomized BAC variable
(i.e. presence of any alcohol), with age, sex, injury severity,
license status, day of week, hour, vehicle role in crash, relation to roadway,
driver's previous incidents, use of restraint as 
potential covariates.
3. Box-Cox transformation of nonzero BAC values.
4. Model selection for nonzero values.
5. Multiple imputation of $M=10$ values using MCMC draws with chains 
started near the maximum likelihood estimates of the imputation model 
parameters followed by back transformation to the original BAC scale.

The results are released in the file 
`miper`, and they are aggregated to the crash level
(the highest BAC among all persons involved in the crash in `miacc` file, 
and the highest BAC among all drivers in `midrvacc` file). 
These files are presented in the wide format
(i.e., each row of the dataset represents one row
of the original data, and the imputations are identified
by suffixes of the column names of the imputed variables,
e.g. `P1` through `P10` in `miper` file).

## Foundations of MI and `mice` package

Multiple imputation seeks to create multiple completed sets of data
that can be analyzed with the "standard" statistical software
for complete data. If the missing data were imputed only once,
the amount of available information that would come from the analysis
of the thus completed data will be overstated, the standard errors
will be too small, and statistical significance overstated.
Mathematics of multiple imputation is calibrated to produce results
that are statistically correct, i.e. the standard errors 
accurately reflect variability in estimates between possible samples,
and tests have appropriate levels.

After $M$ completed data sets are created, the statistical analysis
of the analyst's interest is conducted on each of those data sets,
producing estimates $\hat Q_1, \ldots, \hat Q_M$ along with the
variance-covariance matrices $U_1, \ldots, U_M$. Then the overall
estimate is obtained as

$$
\bar Q = \frac 1M \sum_{l=1}^M Q_l
$$

and the appropriate variance estimate is

$$
T = \bar U + B + \frac 1M B; 
$$

where

$$
\quad \bar U = \frac 1M \sum_{l=1}^M, U_l
$$

is the average of the complete data variances, and

$$
\quad B = \frac 1{M-1} \sum_{l=1}^M (\hat Q_l - \bar Q) (\hat Q_l - \bar Q)'
$$

is variance between imputations. The total variance is the sum
of the conventional sampling error produced on the hypothetical complete data,
extra variance due to the missing data, and the extra simulation variance
caused by the fact that $Q$ has to be estimated. These expressions
are often referred as _Rubin's combining rules_ after the author of the 
multiple imputation concept, Donald Rubin.

Useful descriptive variance ratios are 

$$
\lambda = \frac{B + B/M}{T}
$$

and the relative increase in variance

$$
r = \frac{B + B/M}{\bar U}
$$

When there is no missing data, $B \equiv 0$ and $\lambda=r=0$, thus
indicating the relative extent of how much the missing data is a problem.

For relative small values of $M$, the analyst needs to account for 
the finite _degrees of freedom_. In many statistical analyses, degrees of freedom 
corresponds to the number of independent ways a computed quantity 
(an estimate or a test statistic) could vary from sample to sample, 
for a given way to collect the data and conduct the statistical analysis. 
While in the complete data analysis, the degrees of freedom are usually 
associated with the sample size (often minus the number of estimated parameters), 
for the MI data, the degrees of freedom may be limited by the number of imputations. 
Consider the most extreme scenario when imputation is white noise; 
then the number of ways a statistic computed on this data, assuming 
that the random seeds for data generation are fixed upfront, 
is the number of imputations $M$. 
A low number of degrees of freedom would produce unnecessarily 
long confidence intervals. 

An asymptotic degrees of freedom that relates mostly to the number of imputations is

$$
\nu_1 = \frac{M-1}{\lambda^2}
$$

For complete data with $\lambda=0$, this number is $\infty$, which may not be
satisfactory as the degrees of freedom from the complete analysis (in regression setting) 
is finite at $\nu_{\rm compl} = n-k$ where $n$ is the number of rows of data and 
$k$ is the number of estimated parameters. Then the estimated observed
data degrees of freedom is

$$
\nu_2 = \frac{\nu_{\rm compl} + 1}{\nu_{\rm compl} + 3}\nu_{\rm compl} (1-\lambda)
$$

and hence the combined degrees of freedom that is capped by both is their
geometric average

$$
\nu = \frac{\nu_1 \nu_2}{\nu_1 + \nu_2}
$$

_Fraction of missing information_ (FMI) is commonly defined as 
the ratio of the variance in estimates between imputations to the total variance, 
corrected by degrees of freedom. While the ratio $\lambda$ gives an initial
idea of how much variance is due to missing data, the more accurate
expression also involves the degrees of freedom, and is given by

$$
\gamma = \frac{r + 2/(\nu+3)}{1+r}
$$

A large value of FMI indicates a substantial increase in variance 
due to missing data, and a high level of uncertainty about 
the imputation process (i.e. models that are not accurate enough may 
produce highly biased results).

All of the above expressions are estimate- and parameter-specific,
and may vary substantially from e.g. one predictor variable to another
in regression analysis, depending on the patterns of missing data
and the intercorrelations of variables.


## Requirements of `mice` package workflow

The multiply imputed data sets are not simple rectangular 
data frames. The objects that `mice` package works with are 
[sophisticated data objects](https://amices.org/mice/reference/mids.html) 
enriched with all of the appropriate metadata documenting the missing
data patterns and imputation process
(location of the item missing values in the data set,
imputation models used, covariates used in each model,
information about the imputation chains, random seeds
used by each chain, etc.)

In the [standard `mice` workflow](https://stefvanbuuren.name/fimd/workflow.html),
the researcher: 

1. passes the data set and imputation model definitions to `mice::mice()`,
2. runs the imputation models with `mice::complete()`, 
3. iterates over the imputed data sets using `with()` or `Map()` 
   or `lapply()` or `purrr::map()`, i.e. wrappers around the `for` loops;
4. combines the imputations using Rubin's rules with `mice::pool()`.

While the `mice` package generally prefers to impute the data
itself, it is possible to import externally imputed data
such as FARS, skipping steps 1 and 2. This task is performed
by `mice::as.mids()` function that helps researcher create
a valid `mice` object from the imputed data sets.

## FARS data

```{r get-fars}
if (! file.exists("FARS2022NationalSAS.zip")) {

  fars_url <- "https://static.nhtsa.gov/nhtsa/downloads/FARS/2022/National/FARS2022NationalSAS.zip"
  
  on.exit({
    # remove the downloaded data set
    unlink("FARS2022NationalSAS.zip")
    # remove everything that was unzipped
    unlink("FARS2022NationalSAS", recursive = TRUE)
  })
  
  fars_zip <- download.file(fars_url, file.path(getwd(), "FARS2022NationalSAS.zip") )
  unzip(file.path(getwd(), "FARS2022NationalSAS.zip"))
}

accident <- haven::read_sas(data_file = "FARS2022NationalSAS/accident.sas7bdat", 
            catalog_file="FARS2022NationalSAS/format-Viya/formats.sas7bcat")
vehicle <- haven::read_sas(data_file = "FARS2022NationalSAS/vehicle.sas7bdat", 
            catalog_file="FARS2022NationalSAS/format-Viya/formats.sas7bcat")
person <- haven::read_sas(data_file = "FARS2022NationalSAS/person.sas7bdat", 
            catalog_file="FARS2022NationalSAS/format-Viya/formats.sas7bcat")
miper <- haven::read_sas(data_file = "FARS2022NationalSAS/miper.sas7bdat", 
            catalog_file="FARS2022NationalSAS/format-Viya/formats.sas7bcat")
```


Let us take a look at the FARS person-level data.
The dataset `person` lists all people involved in the registered crashes.
The stable ID variables in this data set can be used to join it
with other datasets in the system: `ST_CASE` uniquely identifies crashes
(within the year of data),
`VEH_NO` identifies vehicles within the crash (0 for non-motorists)
and `PER_NO` identifies individuals within the vehicle / non-motorists
outside any vehicle. 

```{r fars-person}
head(person)
```

The variable `PER_TYP` is the role the person 
played in the crash (1: driver; 2: passenger; 3: occupant of a vehicle
not in transport, 5: pedestrian, 6: bicyclist, 9: unknown occupant
of a vehicle in transport; etc.)

```{r person-pertyp}
person %>% count(PER_TYP) %>% print(n=15)
```

The imputed dataset `miper` only has the IDs necessary for joins,
and imputed values in _wide_ format:

```{r fars-miper}
head(miper)
```

We can verify that imputed values are provided for all active 
traffic participants (drivers, pedestrians, bicyclists, off-duty
commercial vehicle operators, etc.):

```{r fars-miper-pertyp}
full_join(person, miper, by=c("ST_CASE", "VEH_NO", "PER_NO")) %>% 
  mutate(is_imputed = !is.na(P1)) %>% 
  janitor::tabyl(PER_TYP, is_imputed) %>% 
  print(n=15)
```

In the original `person` file, there are several variables related to 
alcohol status:

- `DRINKING` (0: no/alcohol not involved; 1 Yes/alcohol involved; 
   8: not reported; 9: reported as unknown): values 8 and 9 encode missing data;
- `ALC_STATUS` (0: test not given; 2: test given; 8: not reported;
   9: reported as unknown if tested); again values 8 and 9 encode missing data;
- `ATST_TYP`: type of alcohol test;   
- `ALC_RES`: a three digit result code; 
   [BAC](https://en.wikipedia.org/wiki/Blood_alcohol_content) is measured 
   as grams per 100 ml of blood, with values in the range of 0.01-0.20,
   with values above 0.40 potentially fatal. The meaningful values of `ALC_RES`
   are the thousands of the BAC content, so the value `ALC_RES==93` means
   BAC of 0.093 (3 drinks for a woman who weighs 160 lbs/73 kg; 
   4 drinks for a man that size; quite above the legal limit of 0.08 in 
   most US states, way above the legal limit of 0.05 enacted in Utah,
   and the limit of 0.04 for commercial vehicle drivers.)
   
We can verify that for persons eligible for imputation with unknown
alcohol test results, the imputations were made; while for those 
with known results, imputed values are constant. We should expect
that for known values of `ALC_RES` (in single, double and low triple
digits), imputations should not vary, while for the unknown `ALC_RES`
(990s), there should be imputations with non-trivial values. 

```{r fars-imputed-vary}
miper %>%
  rowwise(ST_CASE, VEH_NO, PER_NO ) %>% 
  summarize(imp_sd = sd( c_across(matches("P[0-9]+") ) ) ) %>% 
  mutate(imputations_vary = (imp_sd>0)) %>% 
  ungroup() %>% 
  full_join(person, by=c("ST_CASE", "VEH_NO", "PER_NO")) %>% 
  filter(! PER_TYP %in% c(2, 3, 4)) %>% 
  # count(DRINKING, ALC_STATUS, ALC_RES<900, imputations_vary) %>%
  count(ALC_RES<900, imputations_vary) %>% 
  arrange(desc(n)) 
```

This generally holds, although for a very considerable fraction of the data set
(15K cases out of 70K), imputed values for unknown `ALC_RES` do not vary.
A little bit of investigation reveals that these are imputed as identically zero
between imputations.

```{r fars-imputed-vary-zero}
miper %>%
  rowwise(ST_CASE, VEH_NO, PER_NO ) %>% 
  summarize(imp_sd = sd( c_across(matches("P[0-9]+") ) ), 
            imp_zero = (max( c_across(matches("P[0-9]+") ) ) == 0) ) %>% 
  mutate(imputations_vary = (imp_sd>0)) %>% 
  ungroup() %>% 
  full_join(person, by=c("ST_CASE", "VEH_NO", "PER_NO")) %>% 
  filter(! PER_TYP %in% c(2, 3, 4)) %>% 
  count(ALC_RES<900, imputations_vary, imp_zero) %>% 
  arrange(desc(n)) 
```

In the substantive context, this may be making sense:
commercial drivers should not be expected to be under influence, ever; and 
most drunk driving will probably be happening at night; both vehicle
type and time of crash are predictors in the imputation model, as described above.

## Folding FARS MI data into `mice`

As mentioned above, the functionality to ingest externally imputed data
is provided by `mice::as.mids()` (see compiled documentation at 
https://amices.org/mice/reference/as.mids.html). 
All four inputs of that function needs to be customly specified 
with FARS MI data.

### Case identifiers

We will start with the last argument of `mice::as.mids()`, namely `.id`
which records the single column that identifies cases in the original data.
In FARS `person` file, three combination of three variables identifies the case,
so an appropriate combination of those is needed. 

```{r person-max}
person %>% select(ST_CASE, VEH_NO, PER_NO) %>% summary()
```

Roughly speaking,
`ST_CASE` is 6 digits (the first two are the U.S. state), 
`VEH_NO` is two digits (there are no fatal crashes with 100+ vehicles),
and `PER_NO` is two digits (there are no fatal crashes with 100+ people). 
We can thus produce an identifier stringing the three together:

```{r person-id}
person %>% 
  mutate(person_id = stringr::str_c(
    ST_CASE %>% stringr::str_pad(width=6, side="left", pad="0"),
    VEH_NO  %>% stringr::str_pad(width=2, side="left", pad="0"),
    PER_NO  %>% stringr::str_pad(width=2, side="left", pad="0")
  )) -> person
stopifnot( nrow(person) == nrow(distinct(person, person_id) ) )
```

Note the immediate check that the resulting variable is indeed a unique identifier.

We will repeat the process with other data sets as needed.

### Data set in the long form

The first, `long` argument of `mice::as.mids()` explicitly requires that 
the data are in the long format, i.e. each row of the long data set
is "original observation $\times$ imputation replicate". 
While we reshape the data set to the long format, we will also
rename the relatively uninformative variable `P` to something 
[more powerfully named](https://www.oreilly.com/library/view/code-complete-2nd/0735619670/ch11.html).
We will call the imputation index `imp_index`.

```{r miper-to-long}
miper %>% 
  tidyr::pivot_longer(cols      = matches("P[0-9]+"),
                      names_to  = "imp_index", 
                      names_prefix = "P", names_transform = list(imp_index=as.integer),
                      values_to = "PER_BAC") %>%
  mutate(person_id = stringr::str_c(
    ST_CASE %>% stringr::str_pad(width=6, side="left", pad="0"),
    VEH_NO  %>% stringr::str_pad(width=2, side="left", pad="0"),
    PER_NO  %>% stringr::str_pad(width=2, side="left", pad="0")
  )) %>% 
  select(person_id, imp_index, PER_BAC) -> miper_long
head(miper_long)
```

The `long` argument of `mice::as.mids()` should mirror the output
of `mice::complete(..., action = 'long', include = TRUE)`, so it would need to 

1. have all of the variables in the data set, and
2. have the data sorted by replicate and then by ID. 

Thus we replicate
each line in the data set $M$ times (that number is derived from the `miper_long` dataset.)
The observations with `imp_index==0` should provide the original data set 
with missing values. 
Later, `mice::as.mids()` requires the data to be sorted by imputation index,
so that order is imposed.

```{r person-long}
person %>% 
  distinct(person_id) %>% 
  cross_join(data.frame(imp_index=0:max(miper_long$imp_index))) %>% 
  full_join(person, by="person_id") %>% 
  full_join(miper_long, by=c("person_id", "imp_index")) %>% 
  mutate(PER_BAC = case_when(
    # imputed data
    imp_index > 0                  ~ PER_BAC,
    imp_index == 0 & ALC_RES < 900 ~ ALC_RES/10,
    imp_index == 0 & ALC_RES > 900 ~ as.double(NA),
    TRUE ~ as.double(NA)
  )) %>% 
  arrange(imp_index, person_id) -> person_long
```

Note that `ALC_RES` is on a different scale than imputations
in the `miper` data set: the BAC of 0.07 is coded as `P1==7`
but `ALC_RES==70`. The values are brough down to the scale of 
imputations.

### Missing data patterns in the original data

The second, `where` argument of `mice::as.mids()` needs to identify
the locations in the original dataset where the missing values
are encountered. This has to be carefully constructed for FARS:

  1. all of the existing values in the `person` file should be treated
     as "known" (achieived by `mutate(across(...))` at the bottom of
     the pipe below); 
  2. the values of BAC where imputations are made available
     should be treated as missing
     (achieved by `!is.na(P1)` in the pipe below); 
  3. the missing values that are not eligible for imputation 
     (e.g. passengers, `PER_TYP==2`) can be excluded from
     the imputation exercise (. 
     
Conceptually, it is the `$where` component
of a `mice::mids` object, with value `TRUE` indicating that an imputation
is needed/is made (and hence `P1` is not missing), 
and `FALSE` indicating that no imputation is needed/made
(all regular variables in `person` and ineligible persons 
missing from `miper`).

In creating the version of the `person` data set that aligns with
this requirement, we will add `PER_BAC` column, i.e. named the same 
way as we specified in the `miper_long` data set.

```{r person-where}
full_join(person, miper %>% select(ST_CASE, VEH_NO, PER_NO, P1), 
          by=c("ST_CASE", "VEH_NO", "PER_NO")) %>% 
  mutate(PER_BAC = !is.na(P1)) %>% 
  select(-P1) %>% 
  # deselecting the last two columns: person_id and PER_BAC
  # this is not the best practice as it is sensitive to column order
  mutate(across(.cols = 1:last_col(offset=2L), .fns = ~ FALSE) ) %>% 
  arrange(person_id) -> person_where
```

### Putting it all together

As the `person_id` is consumed by `mice` when processing the data,
the `where` argument of `as.mids()` needs to exclude it:

```{r person_as_mids}
miper_mids <- as.mids(
  long  = person_long,
  where = person_where %>% select(-person_id),
  .imp  = "imp_index",
  .id   = "person_id",
  printFlag = FALSE
)
```

### Analysis and pooling

Suppose our analysis of interest is the average BAC
across hours of the day. The first complication is that
a statistic as simple as the mean with its standard error
is relatively difficult to get: analyses that produce the standard
errors as a part of an R object are usually more complicated.
We can, however, utilize the fact that averages over categories
of a variable are coefficients in a regression with a factor
variable, so we can use syntax like 
`lm(PER_BAC ~ 0 + as.factor(HOUR))` for this analysis,
where the term `+ 0` indicates that the intercept should 
be omitted from the analysis.


The online book "Flexible Imputation of Missing Data"
discusses [several possible workflows](https://stefvanbuuren.name/fimd/workflow.html). 
Workflow 1 is the "base R" flavor using `with()` and `mice::pool()`:

```{r drink_by_time1}
drink_by_hour_run <- with(data = miper_mids, 
                          lm(PER_BAC ~ 0 + as.factor(HOUR)))
drink_by_hour_est1 <- pool(drink_by_hour_run)
drink_by_hour_est1$pooled %>% 
  mutate(term = as.character(term), 
         term = stringr::str_replace_all(term, stringr::fixed("as.factor(HOUR)"), "HOUR==")) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits=3L))) %>% 
  knitr::kable()
```

## Alternative workflow of MI analysis

The above approach ("Workflow 1") makes it rather difficult to customize analysis,
e.g. to restrict it to drivers, among all persons. 
The latter is possible to do so with workflow 7. 
Instead of creating the interim `mids` object,
it deals with the "long" file directly:

```{r drink_by_time7}
person_long %>% 
  # remove the original data
  filter(imp_index > 0) %>% 
  # set the data up by imputation index
  group_by(imp_index) %>%
  # perform complete data analysis
  do(model = lm(formula = PER_BAC ~ 0 + as.factor(HOUR), 
                data = subset(., PER_TYP==1))) %>%
  # convert from data frame of models to a list of models
  as.list() %>% .[[-1]] %>%
  # pool results using Rubin's formulas
  pool() -> drink_by_hour_est7

drink_by_hour_est7$pooled %>% 
  mutate(term = as.character(term), 
         term = stringr::str_replace_all(term, stringr::fixed("as.factor(HOUR)"), "HOUR==")) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits=3L))) %>% 
  knitr::kable()
```

Note that the "original" data have to be excluded from the analysis
with `filter(imp_index>0)`. 

This workflow appears much simpler, aleviating the need for complicated 
intermediate structures such as `where`, although this workflow relies 
on a different set of intermediate steps, including deprecated `dplyr::do()`,
with their own potential complications for other types of analysis.

### Updated workflow with dplyr 1.0.0+

The reference page for [`dplyr::do()`](https://dplyr.tidyverse.org/reference/do.html)
states that its [lifecycle](https://lifecycle.r-lib.org/articles/stages.html) status
is "superseded". It means there is a better alternative although
the old function is not going away (that is the "deprecated" status
where the old function's behavior is considered unsafe, unreliable, or ambiguous.)

The proposed replacement is a combination of 
[`dplyr::nest_by()`](https://dplyr.tidyverse.org/reference/nest_by.html)
which creates a data frame with one row per group,
and `mutate` which creates a column of model results
for `pool()` to work with.
With that replacement, the version of Workflow 7 will be:

```{r drink_by_time8}
person_long %>% 
  # remove the original data
  filter(imp_index > 0) %>% 
  # set the data up by imputation index
  nest_by(imp_index) %>%
  # perform complete data analysis
  mutate(model = list(lm(formula = PER_BAC ~ 0 + as.factor(HOUR), 
                data = (data %>% filter(PER_TYP==1))))) %>%
  # convert from data frame of models to a list of models
  as.list() %>% 
  # pick up the model field from each corresponding model object    
  purrr::pluck("model") %>%
  # pool results using Rubin's formulas
  pool() -> drink_by_hour_est8

drink_by_hour_est8$pooled %>% 
  mutate(term = as.character(term), 
         term = stringr::str_replace_all(term, stringr::fixed("as.factor(HOUR)"), "HOUR==")) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits=3L))) %>% 
  knitr::kable()

testthat::expect_equal(drink_by_hour_est7, drink_by_hour_est8)
```

### Speed up analysis with `future` parallel computation

The imputation task can be sped up considerably (in human time) by using
parallel processing with `future`. Likewise, if the estimation step itself
is slow in the MI pipeline, it can be expedited by parallel processing:

```{r drink_by_time9}
person_long %>% 
  # remove the original data
  filter(imp_index > 0) %>% 
  # set the data up by imputation index
  nest_by(imp_index) %>%
  # perform complete data analysis
  furrr::future_pmap(.f = function(...) {
      the_dots <- rlang::list2(...)
      # implicit return
      lm(formula = PER_BAC ~ 0 + as.factor(HOUR), 
         data = the_dots$data %>% filter(PER_TYP==1))
    }
  ) %>%
  # pool results using Rubin's formulas
  pool() -> drink_by_hour_est9

drink_by_hour_est9$pooled %>% 
  mutate(term = as.character(term), 
         term = stringr::str_replace_all(term, stringr::fixed("as.factor(HOUR)"), "HOUR==")) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits=3L))) %>% 
  knitr::kable()

testthat::expect_equal(drink_by_hour_est7, drink_by_hour_est9)
```

Let us unpack this code a little bit. `furrr::future_pmap()` is a function 
in the `furrr` package which is provides the cross of the mapping 
functionality of `purrr` package (a part of `tidyverse`) with parallel
processing implemented in `future` package. The `future` package
is already used in `mi::mice()` (in older versions, `mi::future_mice()`)
to run imputations in parallel. The `nest_by()` step creates
a tibble with two columns: `imp_index`, the imputation index,
and `data`, tibble corresponding to the data in that imputation.
What `furrr::future_pmap()` (and its sequential computation predecessor,
`purrr::pmap()`) does is it picks up the next line in the data frame
and passes it to the function specified in the `.f` argument.
The entirety of that row is passed as the `...` to `pmap()`,
and the call `.f = function(...)` is simply using that whole row.
As mentioned a few lines above, there are only two columns,
`imp_index` and `data`. We are not making any use of the imputation 
number itself, but we need to get the data to run the estimation
procedure of interest. `rlang::list2(...)` converts the `...` into a list
with named entries corresponding to the columns of the data frame,
i.e. `imp_index` and `data`. Finally, we use `the_dots$data` as
the `data = ` argument of `lm()`, and we apply the filtering operation
to it as needed. The return value of `purrr::pmap()` and `furrr::future_map()`
is a list, and it is ready to be `pool()`ed without any additional
data transformations.

## Interpretation of the MI results

Both sets of estimates point to elevated presence of alcohol late at night.
The estimates do not agree with one another, as they are based on
different subsets. 

There is a subtle story of how imputation worked told by the fraction
of missing information (FMI). In a sense, low levels of BAC in the morning
and in the afternoon are "easy to predict": there are few drunk drivers,
and the missing values of BAC can often be predicted as zeroes. 
While the data on BAC are missing, there is not much uncertainty
as to the BAC levels, which is reflected in low levels of FMI.
(A review of the imputed values of BAC when they were truly missing
in the original data often shows rows of zeroes.)
As drinking ramps up in the evening and at night, the missing values
of BAC may have more variability when the extent of driver's intoxication
cannot be established. FMI rises accordingly.

A combination of the relatively high estimates of BAC with
relatively low level of FMI for the unknown
time of crash (HOUR==99) seem to indicate that these tend to involve alcohol
at night, when law enforcement discovers unconscious bodies 
and has no way of establishing when the crash happened.
The low FMI seems to indicate that the BAC levels encountered
in those crashes that the levels of BAC were not hard to predict
when they were missing.

As a side note, we attempted an alternative to the regression formulation 
of the descriptive statistics found in the package `survey` that
produces the descriptive statistics for complex survey data.
Unfortunately, since output of `svymean()` or `svyby()` or `svyprop()`
is not compatible with `mice::pool()` (or, to be specific, with
the `broom` processing of the results), we could not find a working solution.
